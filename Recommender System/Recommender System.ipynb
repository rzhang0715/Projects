{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System\n",
    "\n",
    "The objective of a Recommender System is to recommend relevant items for users, based on their preference. Recommender system is prevalent in the digital space. For example, when you go shopping on Amazon, you will notice that Amazon is recommending products on the front page before you even type anything in the search box. Similarly, when you go on YouTube, the top bar of Youtube is typically \"videos recommended to you.\" All these features are based on recommmender system. \n",
    "\n",
    "What item to recommend to which user is arguably the most important business decision in many digital platforms. For instance, YouTube cannot control which videos that users upload to it. It cannot control which videos users like to watch. Moreoveor, since watching videos is free, YouTube cannot change the price of its items. It does not have inventory either since each video can be viewed as many times as possible. In this case, what could YouTube control? Or in other words, what differentiates a good video streaming service from a bad one? The answer is recommender system. \n",
    "\n",
    "\n",
    "## Types of Recommender Systems\n",
    "\n",
    "There are **three** types of recommender system.\n",
    "\n",
    "### Popularity-based Recommendation \n",
    "\n",
    "The most obvious system is popularity-based recommendation. In this case, this model recommends to a user the most popular items that the user has not previously consumed. In the movie setting, we will recommend the movie that most users have liked and consumed. In other words, this system utilizes the \"widom of the crowds.\" It usually provides good recommendations for most of the people. Since it is easy to implement, people normally use popularity-based recommendation as a baseline. *Note: this system is not personalized. If both consumers did not watch Movie A and Movie A is the most popular one, both of them will be recommended Movie A.*\n",
    "\n",
    "### Content-based Recommendation \n",
    "\n",
    "This recommender system leverages the data of one customer's historical actions. This recommender systems first utilizes a set of features to describe an item (for example, for movies, we can use the movie's director, main actor, main actress, genre, etc. to describe the movie). When a user comes in, the system will recommend the movies that are closest to the movie that the users have consumed and liked before in terms of the features. For instance, if a user likes action move from Nolan the most, this system will recommend another action movie from Nolan that this user has not consumed. *Note: we will not implement this system in this bonus project since it requires knowledge about supervised learning. We will come back to this topic at the end of this semester.*\n",
    "\n",
    "### Collaborative Filtering Recommendation\n",
    "\n",
    "The last type of recommender system is called collaborative filtering. This approach uses the memory of previous users interactions to compute users similarities based on items they've interacted (user-based approach) or compute items similarities based on the users that have interacted with them (item-based approach).\n",
    "\n",
    "A typical example of this approach is User Neighbourhood-based CF, in which the top-N similar users (usually computed using Pearson correlation) for a user are selected and used to recommend items those similar users liked, but the current user have not interacted yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity-based Recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read-in the preference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_movie_preference():\n",
    "    file_location = \"./data/movie_preference.csv\"\n",
    "    df = pd.read_csv(file_location)\n",
    "    column_names = list(df.columns[1:])\n",
    "    a =[]\n",
    "    b = []\n",
    "    p1 = df.values.tolist() \n",
    "    for i in range(len(p1)):\n",
    "        a.append(p1[i][0])\n",
    "        b.append(p1[i][1:])\n",
    "    preference = dict(zip(a,b))\n",
    "    return [df, column_names, preference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the ranking of most popular movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movies_popularity_ranking(df, movie_names):\n",
    "    movie_popularity_rank = []\n",
    "    \n",
    "    s1 = df.sum(axis = 0)[1:]\n",
    "    s2 = s1.to_frame()\n",
    "    movie_popularity_rank = list(s2[0].rank(ascending = False,method='min'))\n",
    "    \n",
    "    return movie_popularity_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recommendation(movie_popularity_ranking, preference, movie_names, name):\n",
    "    recommended_movie = \"\"\n",
    "    \n",
    "    a = preference[name]\n",
    "    b = []\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == 0:\n",
    "            b.append([movie_names[i],movie_popularity_ranking[i]])\n",
    "    c = sorted(b, key=lambda x:x[1])\n",
    "    recommended_movie= c[0][0]\n",
    "    \n",
    "    return recommended_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-based collaborative recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read-in the preference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_movie_preference():\n",
    "    file_location = \"data/movie_preference.csv\"\n",
    "    df = pd.read_csv(file_location)\n",
    "    column_names = list(df.columns[1:])\n",
    "    a =[]\n",
    "    b = []\n",
    "    p1 = df.values.tolist() \n",
    "    for i in range(len(p1)):\n",
    "        a.append(p1[i][0])\n",
    "        b.append(p1[i][1:])\n",
    "    preference = dict(zip(a,b))\n",
    "    return [df, column_names, preference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the jaccard similarity of any two persons\n",
    "\n",
    "Write a function to compute the jaccard similarity of two persons. In particular, the function should take in two binary vectors representing two persons movie prefecens and compute the jaccard similarity among two persons. In particular, the jaccard similarity of any two persons are equal to \n",
    "\n",
    "$$ \\frac{\\text{Number of Movies both people like}}{\\text{Number of Movies at least one person likes}} $$\n",
    "\n",
    "If there is no movie liked by either of the two persons, jaccard similarity is equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(preference_1, preference_2):\n",
    "    js = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for i in range(len(preference_1)):\n",
    "        if (preference_1[i] == 1) and (preference_2[i] == 1):\n",
    "            a = a+1\n",
    "            b = b+1\n",
    "        elif (preference_1[i] == 1) and (preference_2[i] !=1):\n",
    "            b = b+1\n",
    "        elif (preference_2[i] == 1) and (preference_1[i] !=1):\n",
    "            b = b+1\n",
    "    js = a/b\n",
    "    \n",
    "    \n",
    "    return js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Finding Soulmates\n",
    "Given a person's name, implement a function that finds the person's movie soulmate. Soulmate is defined as the other person who has the highest jaccard similarity that is less than 1 with the focal person. If there are multiple people having the same jaccard similarity with the focal person, pick the person with the smallest name (sorting names in the ascending order). This function should return the soul mate name the movie preference of the soul mate, and the jaccard similarity score of the soul mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Soul_Mate(preference, name):\n",
    "    soulmate = \"\"\n",
    "    soulmate_preference = []\n",
    "    max_js = 0\n",
    "    names = list(df.iloc[:,0])\n",
    "    names.remove(name)\n",
    "    names.sort()\n",
    "    soulmate = names[0]\n",
    "    soulmate_preference = preference[soulmate]\n",
    "    max_js = jaccard_similarity(preference[name],soulmate_preference)\n",
    "    for i in range(len(names)):\n",
    "        n_i = names[i]\n",
    "        if i == 0:\n",
    "            soulmate = names[0]\n",
    "            soulmate_preference = preference[soulmate]\n",
    "            max_js = jaccard_similarity(preference[name],soulmate_preference)\n",
    "        elif i > 0:\n",
    "             if jaccard_similarity(preference[name], preference[n_i]) > max_js:\n",
    "                soulmate = names[i]\n",
    "                soulmate_preference = preference[soulmate] \n",
    "                max_js = jaccard_similarity(preference[name], preference[n_i])\n",
    "    return [soulmate, soulmate_preference, max_js]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Recommendation\n",
    "This function takes in a name and recommends a movie. The recommended movie is the first movie (in the order of the column) that this person's soulmate has watched but this person has not. If such movie does not exist, return an empty string. If it exists, returns the name of the movie.\n",
    "\n",
    "**Note:** from the test case we can see that this recommendation method generates the same outcome as the popularity-based recommendation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recommendation(preference, name, movie_names):\n",
    "    recommendation = \"\"\n",
    "\n",
    "    \n",
    "    recommendation = \"\"\n",
    "    [soulmate, soulmate_preference, js] = Find_Soul_Mate(preference, name)\n",
    "    l1 = len(movie_names)\n",
    "    p1 = preference[name]\n",
    "    p2 = preference[soulmate]\n",
    "    for i in range(l1):\n",
    "        if p1[i] == 0 and p2[i] == 1:\n",
    "            recommendation = movie_names[i]\n",
    "            return recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "In general, Collaborative filtering (CF) is more commonly used than content-based systems because it usually gives better results and is relatively easy to understand (from an overall implementation perspective). The algorithm has the ability to do feature learning on its own, which means that it can start to learn for itself what features to use. \n",
    "\n",
    "CF can be divided into **Memory-Based Collaborative Filtering** and **Model-Based Collaborative filtering**. \n",
    "\n",
    "In this tutorial, we will implement Model-Based CF by using singular value decomposition (SVD) and Memory-Based CF by computing cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "movie_titles = pd.read_csv(\"Movie_Id_Titles\")\n",
    "df = pd.read_csv('u.data', sep='\\t', names=column_names)\n",
    "df = pd.merge(df,movie_titles,on='item_id')\n",
    "\n",
    "n_users = df.user_id.nunique()\n",
    "n_items = df.item_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Recommendation Systems by their very nature are very difficult to evaluate, but we will still show you how to evaluate them in this tutorial. In order to do this, we'll split our data into two sets. However, we won't do our classic X_train,X_test,y_train,y_test split. Instead we can actually just segement the data into two sets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Based Collaborative Filtering\n",
    "\n",
    "Memory-Based Collaborative Filtering approaches can be divided into two main sections: **user-item filtering** and **item-item filtering**. \n",
    "\n",
    "A *user-item filtering* will take a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. \n",
    "\n",
    "In contrast, *item-item filtering* will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations. \n",
    "\n",
    "* *Item-Item Collaborative Filtering*: “Users who liked this item also liked …”\n",
    "* *User-Item Collaborative Filtering*: “Users who are similar to you also liked …”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, you create a user-item matrix which built from the entire dataset.\n",
    "\n",
    "Since we have split the data into testing and training we will need to create two ``[943 x 1682]`` matrices (all users by all movies). \n",
    "\n",
    "The training matrix contains 75% of the ratings and the testing matrix contains 25% of the ratings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have built the user-item matrix you calculate the similarity and create a similarity matrix. \n",
    "\n",
    "The similarity values between items in *Item-Item Collaborative Filtering* are measured by observing all the users who have rated both items.  \n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" style=\"max-width:100%; width: 50%; max-width: none\" src=\"http://s33.postimg.org/i522ma83z/BLOG_CCA_10.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *User-Item Collaborative Filtering* the similarity values between users are measured by observing all the items that are rated by both users.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" style=\"max-width:100%; width: 50%; max-width: none\" src=\"http://s33.postimg.org/mlh3z3z4f/BLOG_CCA_11.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distance metric commonly used in recommender systems is *cosine similarity*, where the ratings are seen as vectors in ``n``-dimensional space and the similarity is calculated based on the angle between these vectors. \n",
    "Cosine similiarity for users *a* and *m* can be calculated using the formula below, where you take dot product of  the user vector *$u_k$* and the user vector *$u_a$* and divide it by multiplication of the Euclidean lengths of the vectors.\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(u_k,u_a)=\\frac{u_k&space;\\cdot&space;u_a&space;}{&space;\\left&space;\\|&space;u_k&space;\\right&space;\\|&space;\\left&space;\\|&space;u_a&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{k,m}x_{a,m}}{\\sqrt{\\sum&space;x_{k,m}^2\\sum&space;x_{a,m}^2}}\"/>\n",
    "\n",
    "To calculate similarity between items *m* and *b* you use the formula:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(i_m,i_b)=\\frac{i_m&space;\\cdot&space;i_b&space;}{&space;\\left&space;\\|&space;i_m&space;\\right&space;\\|&space;\\left&space;\\|&space;i_b&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{a,m}x_{a,b}}{\\sqrt{\\sum&space;x_{a,m}^2\\sum&space;x_{a,b}^2}}\n",
    "\"/>\n",
    "\n",
    "Your first step will be to create the user-item matrix. Since you have both testing and training data you need to create two matrices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the [pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) function from sklearn to calculate the cosine similarity. Note, the output will range from 0 to 1 since the ratings are all positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to make predictions. You have already created similarity matrices: `user_similarity` and `item_similarity` and therefore you can make a prediction by applying following formula for user-based CF:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\bar{x}_{k}&space;&plus;&space;\\frac{\\sum\\limits_{u_a}&space;sim_u(u_k,&space;u_a)&space;(x_{a,m}&space;-&space;\\bar{x_{u_a}})}{\\sum\\limits_{u_a}|sim_u(u_k,&space;u_a)|}\"/>\n",
    "\n",
    "You can look at the similarity between users *k* and *a* as weights that are multiplied by the ratings of a similar user *a* (corrected for the average rating of that user). You will need to normalize it so that the ratings stay between 1 and 5 and, as a final step, sum the average ratings for the user that you are trying to predict. \n",
    "\n",
    "The idea here is that some users may tend always to give high or low ratings to all movies. The relative difference in the ratings that these users give is more important than the absolute values. To give an example: suppose, user *k* gives 4 stars to his favourite movies and 3 stars to all other good movies. Suppose now that another user *t* rates movies that he/she likes with 5 stars, and the movies he/she fell asleep over with 3 stars. These two users could have a very similar taste but treat the rating system differently. \n",
    "\n",
    "When making a prediction for item-based CF you don't need to correct for users average rating since query user itself is used to do predictions.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\frac{\\sum\\limits_{i_b}&space;sim_i(i_m,&space;i_b)&space;(x_{k,b})&space;}{\\sum\\limits_{i_b}|sim_i(i_m,&space;i_b)|}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #You use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "There are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is *Root Mean Squared Error (RMSE)*. \n",
    "<img src=\"https://latex.codecogs.com/gif.latex?RMSE&space;=\\sqrt{\\frac{1}{N}&space;\\sum&space;(x_i&space;-\\hat{x_i})^2}\" title=\"RMSE =\\sqrt{\\frac{1}{N} \\sum (x_i -\\hat{x_i})^2}\" />\n",
    "\n",
    "You can use the [mean_square_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE) function from `sklearn`, where the RMSE is just the square root of MSE. To read more about different evaluation metrics you can take a look at [this article](http://research.microsoft.com/pubs/115396/EvaluationMetrics.TR.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 3.1238762128534643\n",
      "Item-based CF RMSE: 3.45113545077466\n"
     ]
    }
   ],
   "source": [
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-based algorithms are easy to implement and produce reasonable prediction quality. \n",
    "The drawback of memory-based CF is that it doesn't scale to real-world scenarios and doesn't address the well-known cold-start problem, that is when new user or new item enters the system. Model-based CF methods are scalable and can deal with higher sparsity level than memory-based models, but also suffer when new users or items that don't have any ratings enter the system. I would like to thank Ethan Rosenthal for his [post](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/) about Memory-Based Collaborative Filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Collaborative Filtering\n",
    "\n",
    "Model-based Collaborative Filtering is based on **matrix factorization (MF)** which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. \n",
    "When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization you can restructure the  user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector. You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix.\n",
    "\n",
    "Let's calculate the sparsity level of MovieLens dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level of MovieLens100K is 93.7%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(df)/float(n_users*n_items),3)\n",
    "print('The sparsity level of MovieLens100K is ' +  str(sparsity*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example of the learned latent preferences of the users and items: let's say for the MovieLens dataset you have the following information: _(user id, age, location, gender, movie id, director, actor, language, year, rating)_. By applying matrix factorization the model learns that important user features are _age group (under 10, 10-18, 18-30, 30-90)_, _location_ and _gender_, and for movie features it learns that _decade_, _director_ and _actor_ are most important. Now if you look into the information you have stored, there is no such feature as the _decade_, but the model can learn on its own. The important aspect is that the CF model only uses data (user_id, movie_id, rating) to learn the latent features. If there is little data available model-based CF model will predict poorly, since it will be more difficult to learn the latent features. \n",
    "\n",
    "Models that use both ratings and content features are called **Hybrid Recommender Systems** where both Collaborative Filtering and Content-based Models are combined. Hybrid recommender systems usually show higher accuracy than Collaborative Filtering or Content-based Models on their own: they are capable to address the cold-start problem better since if you don't have any ratings for a user or an item you could use the metadata from the user or item to make a prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "A well-known matrix factorization method is **Singular value decomposition (SVD)**. Collaborative Filtering can be formulated by approximating a matrix `X` by using singular value decomposition. The winning team at the Netflix Prize competition used SVD matrix factorization models to produce product recommendations, for more information I recommend to read articles: [Netflix Recommendations: Beyond the 5 stars](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html) and [Netflix Prize and SVD](http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf).\n",
    "The general equation can be expressed as follows:\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?X=USV^T\" title=\"X=USV^T\" />\n",
    "\n",
    "\n",
    "Given `m x n` matrix `X`:\n",
    "* *`U`* is an *`(m x r)`* orthogonal matrix\n",
    "* *`S`* is an *`(r x r)`* diagonal matrix with non-negative real numbers on the diagonal\n",
    "* *V^T* is an *`(r x n)`* orthogonal matrix\n",
    "\n",
    "Elements on the diagnoal in `S` are known as *singular values of `X`*. \n",
    "\n",
    "\n",
    "Matrix *`X`* can be factorized to *`U`*, *`S`* and *`V`*. The *`U`* matrix represents the feature vectors corresponding to the users in the hidden feature space and the *`V`* matrix represents the feature vectors corresponding to the items in the hidden feature space.\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" style=\"max-width:100%; width: 50%; max-width: none\" src=\"http://s33.postimg.org/kwgsb5g1b/BLOG_CCA_5.png\"/>\n",
    "\n",
    "Now you can make a prediction by taking dot product of *`U`*, *`S`* and *`V^T`*.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" style=\"max-width:100%; width: 50%; max-width: none\" src=\"http://s33.postimg.org/ch9lcm6pb/BLOG_CCA_4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 2.715370728004829\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carelessly addressing only the relatively few known entries is highly prone to overfitting. SVD can be very slow and computationally expensive. More recent work minimizes the squared error by applying alternating least square or stochastic gradient descent and uses regularization terms to prevent overfitting. Alternating least square and stochastic gradient descent methods for CF will be covered in the next tutorials.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
